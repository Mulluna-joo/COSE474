{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf336AqU2tuM",
        "outputId": "baeed08a-32a4-4e15-8d44-b582c133d5cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset\n"
      ],
      "metadata": {
        "id": "R0hIRzobDBuB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIEhrkPEC8IV"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_NAMES = ['bottle', 'cable','capsule','carpet']"
      ],
      "metadata": {
        "id": "fBhlKK5RDZky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/Padim/mvtec_anomaly_detection'\n",
        "save_path = '/content/drive/MyDrive/Padim/mvtec_anomaly_detection/mvtec_result'"
      ],
      "metadata": {
        "id": "-sLzI1U0STee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MVTecDataset(Dataset):\n",
        "  def __init__(self, dataset_path=None, class_name= 'metal_nut2', is_train=True, resize=384, cropsize=384):\n",
        "    assert class_name in CLASS_NAMES, 'class_name: {}, should be in {}'.format(class_name,CLASS_NAMES)\n",
        "    self.dataset_path = dataset_path\n",
        "    self.class_name = class_name\n",
        "    self.is_train = is_train\n",
        "    self.resize = resize\n",
        "    self.cropsize = cropsize\n",
        "\n",
        "    #load dataset\n",
        "    self.x, self.y, self.mask = self.load_dataset_folder()\n",
        "\n",
        "    # set transforms\n",
        "    self.transform_x = T.Compose([T.Resize(resize, Image.ANTIALIAS),\n",
        "                                  T.CenterCrop(cropsize),\n",
        "                                  T.ToTensor(),\n",
        "                                  T.Normalize(0.5,0.5)])\n",
        "    self.transform_mask = T.Compose([T.Resize(resize, Image.NEAREST),\n",
        "                                     T.CenterCrop(cropsize),\n",
        "                                     T.ToTensor()])\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    x,y,mask = self.x[idx], self.y[idx], self.mask[idx]\n",
        "\n",
        "    x= Image.open(x).convert('RGB')\n",
        "    x= self.transform_x(x)\n",
        "\n",
        "    if y==0:\n",
        "      mask= torch.zeros([1, self.cropsize, self.cropsize])\n",
        "    else:\n",
        "      mask = Image.open(mask)\n",
        "      mask = self.transform_mask(mask)\n",
        "\n",
        "    return x,y,mask\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "  \n",
        "  def load_dataset_folder(self):\n",
        "    phase = 'train' if self.is_train else 'test'\n",
        "    x,y,mask = [], [], []\n",
        "\n",
        "    img_dir = os.path.join(self.dataset_path, self.class_name, phase)\n",
        "    gt_dir = os.path.join(self.dataset_path, self.class_name, 'ground_truth')\n",
        "\n",
        "    img_types = sorted(os.listdir(img_dir))\n",
        "    for img_type in img_types:\n",
        "\n",
        "      img_type_dir = os.path.join(img_dir, img_type)\n",
        "      if not os.path.isdir(img_type_dir):\n",
        "        continue\n",
        "      img_fpath_list = sorted([os.path.join(img_type_dir,f)\n",
        "                              for f in os.listdir(img_type_dir)])\n",
        "      x.extend(img_fpath_list)\n",
        "\n",
        "      if img_type == 'good':\n",
        "        y.extend([0]* len(img_fpath_list))\n",
        "        mask.extend([None]* len(img_fpath_list))\n",
        "      else :\n",
        "        y.extend([1]* len(img_fpath_list))\n",
        "        gt_type_dir = os.path.join(gt_dir, img_type)\n",
        "        img_fname_list = [os.path.splitext(os.path.basename(f))[0] for f in img_fpath_list]\n",
        "        gt_fpath_list = [os.path.join(gt_type_dir, img_fname + '_mask.png')\n",
        "                        for img_fname in img_fname_list]\n",
        "        mask.extend(gt_fpath_list)\n",
        "\n",
        "    assert len(x) == len(y), 'number of x and y should be same'\n",
        "\n",
        "    return list(x), list(y), list(mask)"
      ],
      "metadata": {
        "id": "W5Z5D10jDvVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "L35CA2HrIDzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from random import sample\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.covariance import LedoitWolf\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from scipy.spatial import distance\n",
        "from skimage import morphology\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import wide_resnet50_2, resnet18\n",
        "\n"
      ],
      "metadata": {
        "id": "Ch67jNchIIlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')"
      ],
      "metadata": {
        "id": "9CFaFb8EO7nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NclLHNGnDMJW",
        "outputId": "99b5e67d-6f25-41d5-8b0d-fabb624c033d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch_pretrained_vit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zrx69iCz2ao",
        "outputId": "0f06382e-867a-4579-f407-34c97782ab76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_pretrained_vit\n",
            "  Downloading pytorch-pretrained-vit-0.0.7.tar.gz (13 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_vit) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->pytorch_pretrained_vit) (4.1.1)\n",
            "Building wheels for collected packages: pytorch-pretrained-vit\n",
            "  Building wheel for pytorch-pretrained-vit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-pretrained-vit: filename=pytorch_pretrained_vit-0.0.7-py3-none-any.whl size=11132 sha256=175d7f4e7f63eeaf261262197af7e6f96abfa0b1e36a666a5645a052e3071e6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/78/4d/447580dd8e95a2aafa0b85820e811568b6b8a8f5460656c0ee\n",
            "Successfully built pytorch-pretrained-vit\n",
            "Installing collected packages: pytorch-pretrained-vit\n",
            "Successfully installed pytorch-pretrained-vit-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_pretrained_vit import ViT\n",
        "#model = ViT('B_16_imagenet1k', pretrained=True)"
      ],
      "metadata": {
        "id": "u-gVH-JpzMrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.transformer.blocks[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5Ewuu2T1gT9",
        "outputId": "e15d5f6a-1487-4bb8-9aa8-0d59a9ae1482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Block(\n",
              "  (attn): MultiHeadedSelfAttention(\n",
              "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (pwff): PositionWiseFeedForward(\n",
              "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "  )\n",
              "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (drop): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vit_modified(ViT):\n",
        "  def forward(self, x):\n",
        "        \"\"\"Breaks image into patches, applies transformer, applies MLP head.\n",
        "        Args:\n",
        "            x (tensor): `b,c,fh,fw`\n",
        "        \"\"\"\n",
        "        b, c, fh, fw = x.shape\n",
        "        x = self.patch_embedding(x)  # b,d,gh,gw\n",
        "        b,d,gh,gw = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)  # b,gh*gw,d\n",
        "        if hasattr(self, 'class_token'):\n",
        "            x = torch.cat((self.class_token.expand(b, -1, -1), x), dim=1)  # b,gh*gw+1,d\n",
        "        if hasattr(self, 'positional_embedding'): \n",
        "            x = self.positional_embedding(x)  # b,gh*gw+1,d \n",
        "        x = self.transformer(x) # b,gh*gw+1,d\n",
        "        x=x.reshape([b,577,24,32])\n",
        "        return x"
      ],
      "metadata": {
        "id": "GVM6V2MBhOAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = MVTecDataset('/content/drive/MyDrive/Padim/mvtec_anomaly_detection', class_name= 'metal_nut2', is_train=False)\n",
        "test_dataloader= DataLoader(test_dataset,batch_size= 32, pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPe7GHuo8yC-",
        "outputId": "247e485f-e900-4ac9-85c9-8f5383c285b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (x, y, mask) in tqdm(test_dataloader, '| feature extraction | test | %s |' % 'bottle'):\n",
        "  print(x.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA8iDUPm9E0c",
        "outputId": "c1874d09-e665-4c0c-a7f8-54e9cc03743c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "| feature extraction | test | bottle |: 100%|██████████| 1/1 [00:23<00:00, 23.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25, 3, 384, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "  parser = argparse.ArgumentParser('PaDim')\n",
        "  parser.add_argument('--data_path', type=str, default='/content/drive/MyDrive/Padim/mvtec_anomaly_detection')\n",
        "  parser.add_argument('--save_path', type=str,default='/content/drive/MyDrive/Padim/mvtec_anomaly_detection/mvtec_result')\n",
        "  parser.add_argument('--arch', type=str, choices=['resnet18','wide_resnet50_2'], default='wide_resnet50_2')\n",
        "  parser.add_argument('-f')\n",
        "  return parser.parse_args()\n",
        "\n",
        "model=''\n",
        "def main():\n",
        "\n",
        "  args= parse_args()\n",
        "  global model\n",
        "  # load model\n",
        "  if args.arch =='resnet18':\n",
        "    model = resnet18(pretrained=True, progress=True)\n",
        "    t_d = 448\n",
        "    d = 100\n",
        "\n",
        "  elif args.arch =='wide_resnet50_2':\n",
        "    model = Vit_modified('B_16_imagenet1k', pretrained=True)\n",
        "    t_d = 1700\n",
        "    d= 550\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  random.seed(1024)\n",
        "  torch.manual_seed(1024)\n",
        "  if use_cuda:\n",
        "    torch.cuda.manual_seed_all(1024)\n",
        "  idx = torch.tensor(sample(range(0,t_d),d))\n",
        "\n",
        "  outputs=[]\n",
        "\n",
        "  def hook(module, input, output):\n",
        "    outputs.append(output)\n",
        "\n",
        "  model.register_forward_hook(hook)\n",
        "  model.register_forward_hook(hook)\n",
        "  model.register_forward_hook(hook)\n",
        "\n",
        "\n",
        "  os.makedirs(os.path.join(args.save_path, 'temp_%s' % args.arch), exist_ok=True)\n",
        "  fig, ax = plt.subplots(1,2,figsize=(20,10))\n",
        "  fig_img_rocauc= ax[0]\n",
        "  fig_pixel_rocauc = ax[1]\n",
        "\n",
        "  total_roc_auc= []\n",
        "  total_pixel_roc_auc = []\n",
        "\n",
        "  for class_name in CLASS_NAMES:\n",
        "    train_dataset = MVTecDataset(args.data_path, class_name= class_name, is_train=True)\n",
        "    train_dataloader= DataLoader(train_dataset,batch_size= 32, pin_memory=True)\n",
        "    test_dataset = MVTecDataset(args.data_path, class_name= class_name, is_train=False)\n",
        "    test_dataloader= DataLoader(test_dataset,batch_size= 32, pin_memory=True)\n",
        "\n",
        "    train_outputs = OrderedDict([('layer1',[]),('layer2',[]),('layer3',[])])\n",
        "    test_outputs = OrderedDict([('layer1',[]),('layer2',[]),('layer3',[])])\n",
        "\n",
        "    # extract feature\n",
        "    train_feature_filepath = os.path.join(args.save_path, 'temp_%s'% args.arch, 'train_%s.pkl'% class_name)\n",
        "    if not os.path.exists(train_feature_filepath):\n",
        "      for (x,_,_) in tqdm(train_dataloader,'| feature extraction| train | %s |' % class_name):\n",
        "        #model prediction\n",
        "        with torch.no_grad():\n",
        "          _ = model(x.to(device))\n",
        "\n",
        "        # get intermediate layer outputs\n",
        "        for k, v in zip(train_outputs.keys(), outputs):\n",
        "          train_outputs[k].append(v.cpu().detach())\n",
        "        # initialize hook outputs\n",
        "        outputs=[]\n",
        "\n",
        "      for k,v in train_outputs.items():\n",
        "        train_outputs[k] = torch.cat(v,0)\n",
        "\n",
        "\n",
        "      # Embedding concat\n",
        "      embedding_vectors = train_outputs['layer1']\n",
        "      for layer_name in ['layer2','layer3']:\n",
        "        embedding_vectors= embedding_concat(embedding_vectors, train_outputs[layer_name])\n",
        "\n",
        "      # randomly select d dimension\n",
        "\n",
        "      embedding_vectors = torch.index_select(embedding_vectors,1,idx)\n",
        "\n",
        "      # calculate multivariate Gaussian distribution\n",
        "      B, C, H, W = embedding_vectors.size()\n",
        "      embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
        "      mean = torch.mean(embedding_vectors, dim=0).numpy()\n",
        "      cov = torch.zeros(C,C,H*W).numpy()\n",
        "      I = np.identity(C)\n",
        "      for i in range(H*W):\n",
        "        cov[:,:,i] = np.cov(embedding_vectors[:,:,i].numpy(),rowvar=False)+0.01 * I\n",
        "\n",
        "      train_outputs = [mean,cov]\n",
        "      with open(train_feature_filepath, 'wb') as f:\n",
        "        pickle.dump(train_outputs,f)\n",
        "    else:\n",
        "      print('load train set feature from: %s' % train_feature_filepath)\n",
        "      with open(train_feature_filepath, 'rb') as f:\n",
        "        train_outputs= pickle.load(f)\n",
        "      \n",
        "    gt_list = []\n",
        "    gt_mask_list = []\n",
        "    test_imgs = []\n",
        "\n",
        "    for (x, y, mask) in tqdm(test_dataloader, '| feature extraction | test | %s |' % class_name):\n",
        "        test_imgs.extend(x.cpu().detach().numpy())\n",
        "        gt_list.extend(y.cpu().detach().numpy())\n",
        "        gt_mask_list.extend(mask.cpu().detach().numpy())\n",
        "        # model prediction\n",
        "        with torch.no_grad():\n",
        "          _ = model(x.to(device))\n",
        "        # get intermediate layer outputs\n",
        "        for k, v in zip(test_outputs.keys(), outputs):\n",
        "            #test_outputs[k].append(v[1].cpu().detach())\n",
        "            test_outputs[k].append(v.cpu().detach())\n",
        "         # initialize hook outputs\n",
        "        outputs = []\n",
        "    for k, v in test_outputs.items():\n",
        "        test_outputs[k] = torch.cat(v, 0)\n",
        "    # Embedding concat\n",
        "    embedding_vectors = test_outputs['layer1']\n",
        "    for layer_name in ['layer2', 'layer3']:\n",
        "        embedding_vectors = embedding_concat(embedding_vectors, test_outputs[layer_name])\n",
        "\n",
        "    # randomly select d dimension\n",
        "    embedding_vectors = torch.index_select(embedding_vectors, 1, idx)\n",
        "    # calculate distance matrix\n",
        "\n",
        "    B,C,H,W = embedding_vectors.size()\n",
        "    embedding_vectors = embedding_vectors.view(B,C,H*W).numpy()\n",
        "    dist_list=[]\n",
        "    print(np.linalg.det(train_outputs[1][:,:,0]))\n",
        "    print(np.linalg.det(train_outputs[1][:,:,1]))\n",
        "    for i in range(H*W):\n",
        "      mean = train_outputs[0][:,i]\n",
        "      dist = [distance.euclidean(sample[:,i],mean) for sample in embedding_vectors]\n",
        "      dist_list.append(dist)\n",
        "\n",
        "    dist_list = np.array(dist_list).transpose(1,0).reshape(B,H,W)\n",
        "\n",
        "    #upsample\n",
        "    dist_list = torch.tensor(dist_list)\n",
        "    score_map = F.interpolate(dist_list.unsqueeze(1), size= x.size(2), mode='bilinear',\n",
        "                                align_corners=False).squeeze().numpy()\n",
        "\n",
        "    for i in range(score_map.shape[0]):\n",
        "      score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
        "\n",
        "    # Normalization\n",
        "\n",
        "    max_score = score_map.max()\n",
        "    min_score = score_map.min()\n",
        "    scores = (score_map- min_score) / (max_score - min_score)\n",
        "\n",
        "    # calculate image-level AUC score\n",
        "\n",
        "    img_scores = scores.reshape(scores.shape[0],-1).max(axis=1)\n",
        "    gt_list = np.asarray(gt_list)\n",
        "    fpr, tpr, _ = roc_curve(gt_list, img_scores)\n",
        "    img_roc_auc = roc_auc_score(gt_list, img_scores)\n",
        "    total_roc_auc.append(img_roc_auc)\n",
        "    print('image ROCAUC: %.3f' % (img_roc_auc))\n",
        "    fig_img_rocauc.plot(fpr,tpr,label= '%s img_ROCAUC: %.3f' % (class_name, img_roc_auc))\n",
        "\n",
        "    #get optimal threshold\n",
        "    gt_mask = np.asarray(gt_mask_list)\n",
        "    precision, recall, thresholds =precision_recall_curve(gt_mask.flatten(),scores.flatten())\n",
        "    a = 2* precision * recall \n",
        "    b = precision + recall\n",
        "    f1 = np.divide(a,b, out= np.zeros_like(a), where= b != 0)\n",
        "    threshold = thresholds[np.argmax(f1)]\n",
        "\n",
        "    # calculate per-pixel level ROCAUC\n",
        "    fpr, tpr, _ = roc_curve(gt_mask.flatten(), scores.flatten())\n",
        "    per_pixel_rocauc = roc_auc_score(gt_mask.flatten(), scores.flatten())\n",
        "    total_pixel_roc_auc.append(per_pixel_rocauc)\n",
        "    print('pixel ROCAUC: %.3f' % (per_pixel_rocauc))\n",
        "\n",
        "    fig_pixel_rocauc.plot(fpr,tpr,label='%s ROCAUC: %.3f' % (class_name, per_pixel_rocauc))\n",
        "    save_dir = args.save_path + '/' + f'pictures_{args.arch}'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    plot_fig(test_imgs, scores, gt_mask_list, threshold, save_dir, class_name)\n",
        "\n",
        "  print('Average pixel ROCAUC: %.3f' % np.mean(total_pixel_roc_auc))\n",
        "  fig_pixel_rocauc.title.set_text('Average pixel ROCAUC: %.3f' % np.mean(total_pixel_roc_auc))\n",
        "  fig_pixel_rocauc.legend(loc='lower right')\n",
        "\n",
        "  fig.tight_layout()\n",
        "  fig.savefig(os.path.join(args.save_path, 'roc_curve.png'), dpi=100)\n",
        "\n",
        "def plot_fig(test_img,scores, gts, threshold, save_dir, class_name):\n",
        "  num = len(scores)\n",
        "  vmax = scores.max()*255.\n",
        "  vmin = scores.min()*255.\n",
        "  for i in range(num):\n",
        "    img = test_img[i]\n",
        "    img = denormalization(img)\n",
        "    gt = gts[i].transpose(1,2,0).squeeze()\n",
        "    heat_map = scores[i] * 255\n",
        "    mask = scores[i]\n",
        "    mask[mask > threshold] = 1\n",
        "    mask[mask <= threshold] = 0\n",
        "    kernel = morphology.disk(4)\n",
        "    mask= morphology.opening(mask, kernel)\n",
        "    mask *= 255\n",
        "    vis_img = mark_boundaries(img, mask, color=(1,0,0), mode= 'thick')\n",
        "    fig_img, ax_img = plt.subplots(1,5,figsize=(12,3))\n",
        "    fig_img.subplots_adjust(right=0.9)\n",
        "    norm = matplotlib.colors.Normalize(vmin=vmin,vmax=vmax)\n",
        "    for ax_i in ax_img:\n",
        "      ax_i.axes.xaxis.set_visible(False)\n",
        "      ax_i.axes.yaxis.set_visible(False)\n",
        "    ax_img[0].imshow(img)\n",
        "    ax_img[0].title.set_text('Image')\n",
        "    ax_img[1].imshow(gt, cmap='gray')\n",
        "    ax_img[1].title.set_text('GroundTruth')\n",
        "    ax = ax_img[2].imshow(heat_map, cmap='jet', norm=norm)\n",
        "    ax_img[2].imshow(img,cmap='gray',interpolation='none')\n",
        "    ax_img[2].title.set_text('Predicted heat map')\n",
        "    ax_img[3].imshow(mask, cmap='gray')\n",
        "    ax_img[3].title.set_text('Predicted mask')\n",
        "    ax_img[4].imshow(vis_img)\n",
        "    ax_img[4].title.set_text('Segmentation result')\n",
        "    left=0.92\n",
        "    bottom=0.15\n",
        "    width = 0.015\n",
        "    height = 1-2*bottom\n",
        "    rect= [left, bottom, width, height]\n",
        "    cbar_ax = fig_img.add_axes(rect)\n",
        "    cb = plt.colorbar(ax, shrink=0.6, cax=cbar_ax, fraction=0.046)\n",
        "    cb.ax.tick_params(labelsize=8)\n",
        "    font={'family': 'serif','color': 'black','weight': 'normal','size': 8}\n",
        "    cb.set_label('Anomaly Score', fontdict=font)\n",
        "\n",
        "    fig_img.savefig(os.path.join(save_dir,class_name+ '_{}'.format(i)),dpi=100)\n",
        "    plt.close()\n",
        "    \n",
        "def denormalization(x):\n",
        "  mean= np.array([0.485,0.456,0.406])\n",
        "  std= np.array([0.229,0.224,0.225])\n",
        "  x= (((x.transpose(1,2,0)*std)+mean)*255.).astype(np.uint8)\n",
        "\n",
        "  return x\n",
        "\n",
        "def embedding_concat(x,y):\n",
        "  B,C1,H1,W1 = x.size()\n",
        "  _,C2,H2,W2 = y.size()\n",
        "  s= int(H1/H2)\n",
        "  x = F.unfold(x, kernel_size=s, dilation=1, stride=s)\n",
        "  x = x.view(B,C1, -1,H2,W2)\n",
        "  z= torch.zeros(B, C1+C2,x.size(2),H2,W2)\n",
        "  for i in range(x.size(2)):\n",
        "    z[:,:,i,:,:]= torch.cat((x[:,:,i,:,:],y),1)\n",
        "  z = z.view(B,-1,H2*W2)\n",
        "  z = F.fold(z, kernel_size=s, output_size=(H1,W1),stride=s)\n",
        "\n",
        "  return z\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "oawicp6iLwIF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "41669107-07ce-48d2-a627-8099a7ef6d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights.\n",
            "load train set feature from: /content/drive/MyDrive/Padim/mvtec_anomaly_detection/mvtec_result/temp_wide_resnet50_2/train_metal_nut2.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "| feature extraction | test | metal_nut2 |: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LinAlgError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5fecb563748e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-5fecb563748e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m       \u001b[0mconv_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m       \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmahalanobis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv_inv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membedding_vectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0;31m#dist = [distance.euclidean(sample[:,i],mean) for sample in embedding_vectors]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}